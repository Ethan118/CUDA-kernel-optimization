{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFuxPWe8PnM_",
        "outputId": "6f28c5dc-ac92-4dc5-b094-94a1a2e1f184"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Oct 22 03:13:45 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   58C    P8    12W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# This should say you're using a GPU.\n",
        "# If you aren't using a GPU, go to \"Runtime\",\n",
        "# then select \"change runtime type\" and click\n",
        "# T4 GPU.\n",
        "\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0r8ZHguJRTKj",
        "outputId": "10538aa1-54b9-47fe-f549-f58022129f85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'kernel_tuner'...\n",
            "remote: Enumerating objects: 9199, done.\u001b[K\n",
            "remote: Counting objects: 100% (1570/1570), done.\u001b[K\n",
            "remote: Compressing objects: 100% (373/373), done.\u001b[K\n",
            "remote: Total 9199 (delta 1338), reused 1316 (delta 1194), pack-reused 7629\u001b[K\n",
            "Receiving objects: 100% (9199/9199), 4.25 MiB | 16.88 MiB/s, done.\n",
            "Resolving deltas: 100% (6579/6579), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/WAT-ai/kernel_tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfYGDguKSan6",
        "outputId": "1045b72c-df9c-4609-84c8-c1874deeef6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kernel_tuner[cuda]\n",
            "  Downloading kernel_tuner-0.4.5-py3-none-any.whl (122 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.5/122.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<1.24.0,>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from kernel_tuner[cuda]) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from kernel_tuner[cuda]) (1.11.3)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from kernel_tuner[cuda]) (4.19.1)\n",
            "Collecting python-constraint (from kernel_tuner[cuda])\n",
            "  Downloading python-constraint-1.4.0.tar.bz2 (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting xmltodict (from kernel_tuner[cuda])\n",
            "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
            "Collecting pycuda (from kernel_tuner[cuda])\n",
            "  Downloading pycuda-2022.2.2.tar.gz (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting nvidia-ml-py (from kernel_tuner[cuda])\n",
            "  Downloading nvidia_ml_py-12.535.108-py3-none-any.whl (36 kB)\n",
            "Collecting pynvml>=11.4.1 (from kernel_tuner[cuda])\n",
            "  Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->kernel_tuner[cuda]) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->kernel_tuner[cuda]) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->kernel_tuner[cuda]) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->kernel_tuner[cuda]) (0.10.6)\n",
            "Collecting pytools>=2011.2 (from pycuda->kernel_tuner[cuda])\n",
            "  Downloading pytools-2023.1.1-py2.py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: appdirs>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from pycuda->kernel_tuner[cuda]) (1.4.4)\n",
            "Collecting mako (from pycuda->kernel_tuner[cuda])\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda->kernel_tuner[cuda]) (3.11.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pytools>=2011.2->pycuda->kernel_tuner[cuda]) (4.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from mako->pycuda->kernel_tuner[cuda]) (2.1.3)\n",
            "Building wheels for collected packages: pycuda, python-constraint\n",
            "  Building wheel for pycuda (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycuda: filename=pycuda-2022.2.2-cp310-cp310-linux_x86_64.whl size=661265 sha256=a0a625a19b96dee116909ffe20ed6bae878f31b4f1c6bddadab02464fcdaa378\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/7b/06/82a395a243fce00035dea9914d92bbef0013401497d849f8bc\n",
            "  Building wheel for python-constraint (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-constraint: filename=python_constraint-1.4.0-py2.py3-none-any.whl size=24058 sha256=8f8778825008f2947675ca75cbdb18a3083cd44ce757bbb6e2879155f59155d3\n",
            "  Stored in directory: /root/.cache/pip/wheels/2e/f2/2b/cb08b5fe129e4f69b7033061f256e5c551b0aa1160c2872aee\n",
            "Successfully built pycuda python-constraint\n",
            "Installing collected packages: python-constraint, nvidia-ml-py, xmltodict, pytools, pynvml, mako, pycuda, kernel_tuner\n",
            "Successfully installed kernel_tuner-0.4.5 mako-1.2.4 nvidia-ml-py-12.535.108 pycuda-2022.2.2 pynvml-11.5.0 python-constraint-1.4.0 pytools-2023.1.1 xmltodict-0.13.0\n"
          ]
        }
      ],
      "source": [
        "pip install kernel_tuner[cuda]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Demo: Enhancing Convolution Operations with Correctness Verification\n",
        "\n",
        "In the context of computational operations utilizing convolution kernels, this example operates on a framework that largely mirrors a standard Convolution example. However, a nuanced distinction emerges in the strategic utilization of a naive kernel to establish a reference output. This output subsequently assumes a pivotal role, serving as a benchmark to verify the correctness of each kernel before its performance is benchmarked.\n",
        "\n",
        "Here's a breakdown of the process without code examples:\n",
        "\n",
        "1. **Kernel and Data Initialization**:\n",
        "    - The kernel code is fetched from a predefined file and various data, including filter sizes and problem sizes, is initialized.\n",
        "    - Input data and filters are set up and computational arguments are defined to facilitate subsequent operations.\n",
        "  \n",
        "2. **Parameter Tuning Setup**:\n",
        "    - A structured collection of tuning parameters is configured, comprising diverse block and tile sizes.\n",
        "    - Options to utilize or omit padding and the read-only cache are incorporated, providing versatile tuning capabilities.\n",
        "\n",
        "3. **Reference Output Generation**:\n",
        "    - A naive convolution kernel is executed, wielding pre-specified parameters, to generate a reference output.\n",
        "    - This output is crucial as it functions as a benchmark for accuracy in subsequent operations.\n",
        "\n",
        "4. **Verification and Kernel Tuning**:\n",
        "    - An 'answer' list is set up, which marks input data as `None` and utilizes non-`None` data for verification purposes.\n",
        "    - The `tune_kernel` function is invoked to tune the kernel, parallely cross-verifying each kernel in the parameter space against the pre-established reference output.\n",
        "   \n",
        "This methodology pivots around utilizing a naive kernel to derive a reference output and subsequently leveraging it for correctness checks during convolution operations, all before performance benchmarking occurs. This ensures computational reliability as it guarantees that only kernels which have been validated for correctness are subjected to the tuning and optimization phase."
      ],
      "metadata": {
        "id": "idUV5FnqA2-z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrnSj1WGRcgQ",
        "outputId": "ea5028e5-6cc1-4a11-f1ae-a3d77f99a846"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using: Tesla T4\n",
            "Using: Tesla T4\n",
            "filter_width=17, filter_height=17, block_size_x=32, block_size_y=8, tile_size_x=1, tile_size_y=2, use_padding=0, read_only=1, time=8.863ms\n",
            "filter_width=17, filter_height=17, block_size_x=64, block_size_y=16, tile_size_x=1, tile_size_y=4, use_padding=1, read_only=1, time=13.511ms\n",
            "filter_width=17, filter_height=17, block_size_x=80, block_size_y=4, tile_size_x=1, tile_size_y=4, use_padding=1, read_only=0, time=5.722ms\n",
            "filter_width=17, filter_height=17, block_size_x=16, block_size_y=16, tile_size_x=1, tile_size_y=1, use_padding=1, read_only=1, time=14.330ms\n",
            "filter_width=17, filter_height=17, block_size_x=48, block_size_y=4, tile_size_x=1, tile_size_y=4, use_padding=1, read_only=1, time=5.216ms\n",
            "max_fevals reached\n",
            "best performing configuration:\n",
            "filter_width=17, filter_height=17, block_size_x=48, block_size_y=4, tile_size_x=1, tile_size_y=4, use_padding=1, read_only=1, time=5.216ms\n",
            "([OrderedDict([('filter_width', 17), ('filter_height', 17), ('block_size_x', 32), ('block_size_y', 8), ('tile_size_x', 1), ('tile_size_y', 2), ('use_padding', 0), ('read_only', 1), ('time', 8.862820761544365), ('times', [8.890368461608887, 8.863776206970215, 8.860960006713867, 8.858783721923828, 8.855680465698242, 8.85366439819336, 8.856512069702148]), ('compile_time', 0.29524100000344333), ('verification_time', 184.1243220000024), ('benchmark_time', 62.51859299993612), ('strategy_time', 35.44167899997319), ('framework_time', 0.5943410001236771), ('timestamp', '2023-10-16 02:29:52.050607+00:00')]), OrderedDict([('filter_width', 17), ('filter_height', 17), ('block_size_x', 64), ('block_size_y', 16), ('tile_size_x', 1), ('tile_size_y', 4), ('use_padding', 1), ('read_only', 1), ('time', 13.51130975995745), ('times', [13.537311553955078, 13.516096115112305, 13.504511833190918, 13.504511833190918, 13.506560325622559, 13.50534439086914, 13.50483226776123]), ('compile_time', 2365.0591659999236), ('verification_time', 182.05592700007855), ('benchmark_time', 95.16986500000257), ('strategy_time', 0.009852000061982835), ('framework_time', 0.6846459998541832), ('timestamp', '2023-10-16 02:29:54.693623+00:00')]), OrderedDict([('filter_width', 17), ('filter_height', 17), ('block_size_x', 80), ('block_size_y', 4), ('tile_size_x', 1), ('tile_size_y', 4), ('use_padding', 1), ('read_only', 0), ('time', 5.72160005569458), ('times', [5.755296230316162, 5.712063789367676, 5.71395206451416, 5.711008071899414, 5.707968235015869, 5.713632106781006, 5.737279891967773]), ('compile_time', 1107.358481999995), ('verification_time', 182.22358100001657), ('benchmark_time', 40.70555299995249), ('strategy_time', 0.0070759999744041124), ('framework_time', 0.7092750000765591), ('timestamp', '2023-10-16 02:29:56.024657+00:00')]), OrderedDict([('filter_width', 17), ('filter_height', 17), ('block_size_x', 16), ('block_size_y', 16), ('tile_size_x', 1), ('tile_size_y', 1), ('use_padding', 1), ('read_only', 1), ('time', 14.33042744227818), ('times', [14.344736099243164, 14.34006404876709, 14.336031913757324, 14.325759887695312, 14.325599670410156, 14.32579231262207, 14.315008163452148]), ('compile_time', 498.89615400002185), ('verification_time', 190.63346000007186), ('benchmark_time', 101.00363499998366), ('strategy_time', 0.007165999932112754), ('framework_time', 0.6826900000760361), ('timestamp', '2023-10-16 02:29:56.815909+00:00')]), OrderedDict([('filter_width', 17), ('filter_height', 17), ('block_size_x', 48), ('block_size_y', 4), ('tile_size_x', 1), ('tile_size_y', 4), ('use_padding', 1), ('read_only', 1), ('time', 5.2155658176967075), ('times', [5.234720230102539, 5.209887981414795, 5.206240177154541, 5.201920032501221, 5.205535888671875, 5.213856220245361, 5.236800193786621]), ('compile_time', 1078.0329049999864), ('verification_time', 180.4331349999302), ('benchmark_time', 37.14129699994828), ('strategy_time', 0.007029000016700593), ('framework_time', 1.1902810001629405), ('timestamp', '2023-10-16 02:29:58.112743+00:00')])], {'device_name': 'Tesla T4', 'cuda_version': 12000, 'compute_capability': '75', 'iterations': 7, 'compiler_options': None, 'device_properties': {'AsyncEngineCount': 3, 'CanFlushRemoteWrites': 0, 'CanMapHostMemory': 1, 'CanUseHostPointerForRegisteredMem': 1, 'ClockRate': 1590000, 'ComputeMode': 0, 'ComputePreemptionSupported': 1, 'ConcurrentKernels': 1, 'ConcurrentManagedAccess': 1, 'CooperativeLaunch': 1, 'CooperativeMultiDeviceLaunch': 1, 'DirectManagedMemAccessFromHost': 0, 'EccEnabled': 1, 'GPUDirectRDMAFlushWritesOptions': 1, 'GPUDirectRDMASupported': 1, 'GPUDirectRDMAWritesOrdering': 0, 'GlobalL1CacheSupported': 1, 'GlobalMemoryBusWidth': 256, 'GpuOverlap': 1, 'HostNativeAtomicSupported': 0, 'HostRegisterReadOnlySupported': 1, 'HostRegisterSupported': 1, 'Integrated': 0, 'IsMultiGpuBoard': 0, 'KernelExecTimeout': 0, 'L2CacheSize': 4194304, 'LocalL1CacheSupported': 1, 'ManagedMemory': 1, 'MaxBlockDimX': 1024, 'MaxBlockDimY': 1024, 'MaxBlockDimZ': 64, 'MaxBlocksPerMultiprocessor': 16, 'MaxGridDimX': 2147483647, 'MaxGridDimY': 65535, 'MaxGridDimZ': 65535, 'MaxPitch': 2147483647, 'MaxRegistersPerBlock': 65536, 'MaxRegistersPerMultiprocessor': 65536, 'MaxSharedMemoryPerBlock': 49152, 'MaxSharedMemoryPerBlockOptin': 65536, 'MaxSharedMemoryPerMultiprocessor': 65536, 'MaxSurface1DLayeredLayers': 2048, 'MaxSurface1DLayeredWidth': 32768, 'MaxSurface1DWidth': 32768, 'MaxSurface2DHeight': 65536, 'MaxSurface2DLayeredHeight': 32768, 'MaxSurface2DLayeredLayers': 2048, 'MaxSurface2DLayeredWidth': 32768, 'MaxSurface2DWidth': 131072, 'MaxSurface3DDepth': 16384, 'MaxSurface3DHeight': 16384, 'MaxSurface3DWidth': 16384, 'MaxSurfaceCubemapLayeredLayers': 2046, 'MaxSurfaceCubemapLayeredWidth': 32768, 'MaxSurfaceCubemapWidth': 32768, 'MaxTexture1DLayeredLayers': 2048, 'MaxTexture1DLayeredWidth': 32768, 'MaxTexture1DLinearWidth': 268435456, 'MaxTexture1DMipmappedWidth': 32768, 'MaxTexture1DWidth': 131072, 'MaxTexture2DGatherHeight': 32768, 'MaxTexture2DGatherWidth': 32768, 'MaxTexture2DHeight': 65536, 'MaxTexture2DLayeredHeight': 32768, 'MaxTexture2DLayeredLayers': 2048, 'MaxTexture2DLayeredWidth': 32768, 'MaxTexture2DLinearHeight': 65000, 'MaxTexture2DLinearPitch': 2097120, 'MaxTexture2DLinearWidth': 131072, 'MaxTexture2DMipmappedHeight': 32768, 'MaxTexture2DMipmappedWidth': 32768, 'MaxTexture2DWidth': 131072, 'MaxTexture3DDepth': 16384, 'MaxTexture3DDepthAlt': 32768, 'MaxTexture3DHeight': 16384, 'MaxTexture3DHeightAlt': 8192, 'MaxTexture3DWidth': 16384, 'MaxTexture3DWidthAlt': 8192, 'MaxTextureCubemapLayeredLayers': 2046, 'MaxTextureCubemapLayeredWidth': 32768, 'MaxTextureCubemapWidth': 32768, 'MaxThreadsPerBlock': 1024, 'MaxThreadsPerMultiProcessor': 1024, 'MaxTimelineSemaphoreInteropSupported': 1, 'MemoryClockRate': 5001000, 'MemoryPoolSupportedHandleTypes': 1, 'MemoryPoolsSupported': 1, 'MultiGpuBoardGroupID': 0, 'MultiProcessorCount': 40, 'PageableMemoryAccess': 0, 'PageableMemoryAccessUsesHostPageTables': 0, 'PciBusId': 0, 'PciDeviceId': 4, 'PciDomainId': 0, 'Reserved92': 0, 'Reserved93': 0, 'Reserved94': 0, 'ReservedSharedMemoryPerBlock': 0, 'SingleToDoublePrecisionPerfRatio': 32, 'SparseCudaArraySupported': 1, 'StreamPrioritiesSupported': 1, 'SurfaceAlignment': 512, 'TccDriver': 0, 'TextureAlignment': 512, 'TexturePitchAlignment': 32, 'TotalConstantMemory': 65536, 'UnifiedAddressing': 1, 'WarpSize': 32}, 'total_framework_time': 3.861233000293396, 'total_strategy_time': 35.47280199995839, 'total_compile_time': 5049.64194799993, 'total_verification_time': 919.4704250000996, 'total_benchmark_time': 336.5389429998231, 'overhead_time': 1037.4963079999588})\n",
            "\n",
            " Actual time used: 13098.470703125\n"
          ]
        }
      ],
      "source": [
        "import numpy\n",
        "import kernel_tuner\n",
        "from collections import OrderedDict\n",
        "from kernel_tuner.strategies import *\n",
        "\n",
        "def tune():\n",
        "    with open(\"kernel_tuner/examples/cuda/convolution.cu\", \"r\") as f:\n",
        "        kernel_string = f.read()\n",
        "\n",
        "    filter_size = (17, 17)\n",
        "    problem_size = (4096, 4096)\n",
        "    size = numpy.prod(problem_size)\n",
        "    border_size = (filter_size[0] // 2 * 2, filter_size[1] // 2 * 2)\n",
        "    input_size = (problem_size[0] + border_size[0]) * (problem_size[1] + border_size[1])\n",
        "\n",
        "    output = numpy.zeros(size).astype(numpy.float32)\n",
        "    input = numpy.random.randn(input_size).astype(numpy.float32)\n",
        "\n",
        "    filter = numpy.random.randn(filter_size[0] * filter_size[1]).astype(numpy.float32)\n",
        "    cmem_args = {\"d_filter\": filter}\n",
        "\n",
        "    args = [output, input, filter]\n",
        "    tune_params = OrderedDict()\n",
        "    tune_params[\"filter_width\"] = [filter_size[0]]\n",
        "    tune_params[\"filter_height\"] = [filter_size[1]]\n",
        "\n",
        "    # tune_params[\"block_size_x\"] = [16*i for i in range(1,3)]\n",
        "    tune_params[\"block_size_x\"] = [16 * i for i in range(1, 9)]\n",
        "    # tune_params[\"block_size_y\"] = [2**i for i in range(1,5)]\n",
        "    tune_params[\"block_size_y\"] = [2**i for i in range(1, 6)]\n",
        "\n",
        "    tune_params[\"tile_size_x\"] = [2**i for i in range(3)]\n",
        "    tune_params[\"tile_size_y\"] = [2**i for i in range(3)]\n",
        "\n",
        "    tune_params[\"use_padding\"] = [\n",
        "        0,\n",
        "        1,\n",
        "    ]  # toggle the insertion of padding in shared memory\n",
        "    tune_params[\"read_only\"] = [0, 1]  # toggle using the read-only cache\n",
        "\n",
        "    grid_div_x = [\"block_size_x\", \"tile_size_x\"]\n",
        "    grid_div_y = [\"block_size_y\", \"tile_size_y\"]\n",
        "\n",
        "    # compute the answer using a naive kernel\n",
        "    params = {\"block_size_x\": 16, \"block_size_y\": 16}\n",
        "    tune_params[\"filter_width\"] = [filter_size[0]]\n",
        "    tune_params[\"filter_height\"] = [filter_size[1]]\n",
        "    results = kernel_tuner.run_kernel(\n",
        "        \"convolution_naive\",\n",
        "        kernel_string,\n",
        "        problem_size,\n",
        "        args,\n",
        "        params,\n",
        "        grid_div_y=[\"block_size_y\"],\n",
        "        grid_div_x=[\"block_size_x\"],\n",
        "        lang=\"cupy\",\n",
        "    )\n",
        "\n",
        "    # set non-output fields to None\n",
        "    answer = [results[0], None, None]\n",
        "\n",
        "    # start kernel tuning with correctness verification\n",
        "    return kernel_tuner.tune_kernel(\n",
        "        \"convolution_kernel\",\n",
        "        kernel_string,\n",
        "        problem_size,\n",
        "        args,\n",
        "        tune_params,\n",
        "        grid_div_y=grid_div_y,\n",
        "        grid_div_x=grid_div_x,\n",
        "        verbose=True,\n",
        "        cmem_args=cmem_args,\n",
        "        answer=answer,\n",
        "        lang=\"cupy\",\n",
        "        strategy=\"genetic_algorithm\",\n",
        "        strategy_options=dict(max_fevals=5)\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import time\n",
        "\n",
        "    s1 = time.time() * 1000\n",
        "    results = tune()\n",
        "    print(results)\n",
        "\n",
        "    e1 = time.time() * 1000\n",
        "    print(\"\\n Actual time used:\", e1 - s1)\n",
        "    import json\n",
        "\n",
        "    with open(\"convolution_gpu_runtime.json\", \"w\") as fp:\n",
        "        json.dump(results, fp)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Strategy\n",
        "\n",
        "This is where we will be tuning our reinforcement learning strategy to optimize the demo kernel.\n",
        "\n",
        "Demo Kernel 1: Vector Add\n",
        "\n"
      ],
      "metadata": {
        "id": "PbpF6rk2Yjo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# default imports to get started\n",
        "from kernel_tuner import util\n",
        "from kernel_tuner.searchspace import Searchspace\n",
        "from kernel_tuner.strategies import common\n",
        "from kernel_tuner.strategies.common import CostFunc\n",
        "\n",
        "# NOTE: Q-learning strategy"
      ],
      "metadata": {
        "id": "qLHOoMnwY7PN"
      },
      "execution_count": 5,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}